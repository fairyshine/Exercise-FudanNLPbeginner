{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "train_file='Dataset/train.txt'\n",
    "test_file='Dataset/test.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 处理训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset=[]\n",
    "with open(train_file,'r') as f:\n",
    "    LS_list=[]\n",
    "    trainData={}\n",
    "    for line in f:\n",
    "        if line != '\\n':\n",
    "            LS_list.append(line.split())\n",
    "        else:\n",
    "            trainData['text']=LS_list[0][0]\n",
    "            trainData['POS']=LS_list[0][1]\n",
    "            trainData['Chunk']=LS_list[0][2]\n",
    "            trainData['NER']=LS_list[0][3]\n",
    "            for i in range(1,len(LS_list)):\n",
    "                LS=LS_list[i]\n",
    "                trainData['text']=trainData['text']+' '+LS[0]\n",
    "                trainData['POS']=trainData['POS']+' '+LS[1]\n",
    "                trainData['Chunk']=trainData['Chunk']+' '+LS[2]\n",
    "                trainData['NER']=trainData['NER']+' '+LS[3]\n",
    "            trainDataset.append(trainData)\n",
    "            LS_list=[]\n",
    "            trainData={}\n",
    "\n",
    "    trainData['text']=LS_list[0][0]\n",
    "    trainData['POS']=LS_list[0][1]\n",
    "    trainData['Chunk']=LS_list[0][2]\n",
    "    trainData['NER']=LS_list[0][3]\n",
    "    for i in range(1,len(LS_list)):\n",
    "        LS=LS_list[i]\n",
    "        trainData['text']=trainData['text']+' '+LS[0]\n",
    "        trainData['POS']=trainData['POS']+' '+LS[1]\n",
    "        trainData['Chunk']=trainData['Chunk']+' '+LS[2]\n",
    "        trainData['NER']=trainData['NER']+' '+LS[3]\n",
    "    trainDataset.append(trainData)\n",
    "    LS_list=[]\n",
    "    trainData={}\n",
    "\n",
    "with open('Dataset/train.jsonl','w') as f:\n",
    "    for data in trainDataset:\n",
    "        json.dump(data, f,ensure_ascii=False)\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 处理测试集（同理1.1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataset=[]\n",
    "with open(test_file,'r') as f:\n",
    "    LS_list=[]\n",
    "    testData={}\n",
    "    for line in f:\n",
    "        if line != '\\n':\n",
    "            LS_list.append(line.split())\n",
    "        else:\n",
    "            testData['text']=LS_list[0][0]\n",
    "            testData['POS']=LS_list[0][1]\n",
    "            testData['Chunk']=LS_list[0][2]\n",
    "            testData['NER']=LS_list[0][3]\n",
    "            for i in range(1,len(LS_list)):\n",
    "                LS=LS_list[i]\n",
    "                testData['text']=testData['text']+' '+LS[0]\n",
    "                testData['POS']=testData['POS']+' '+LS[1]\n",
    "                testData['Chunk']=testData['Chunk']+' '+LS[2]\n",
    "                testData['NER']=testData['NER']+' '+LS[3]\n",
    "            testDataset.append(testData)\n",
    "            LS_list=[]\n",
    "            testData={}\n",
    "\n",
    "    testData['text']=LS_list[0][0]\n",
    "    testData['POS']=LS_list[0][1]\n",
    "    testData['Chunk']=LS_list[0][2]\n",
    "    testData['NER']=LS_list[0][3]\n",
    "    for i in range(1,len(LS_list)):\n",
    "        LS=LS_list[i]\n",
    "        testData['text']=testData['text']+' '+LS[0]\n",
    "        testData['POS']=testData['POS']+' '+LS[1]\n",
    "        testData['Chunk']=testData['Chunk']+' '+LS[2]\n",
    "        testData['NER']=testData['NER']+' '+LS[3]\n",
    "    testDataset.append(testData)\n",
    "    LS_list=[]\n",
    "    testData={}\n",
    "\n",
    "with open('Dataset/test.jsonl','w') as f:\n",
    "    for data in testDataset:\n",
    "        json.dump(data, f,ensure_ascii=False)\n",
    "        f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset=[]\n",
    "phraseList=[]\n",
    "\n",
    "with open('Dataset/train.jsonl','r') as f:\n",
    "    for line in f:\n",
    "        Dataset.append(json.loads(line));\n",
    "\n",
    "with open('Dataset/test.jsonl','r') as f:\n",
    "    for line in f:\n",
    "        Dataset.append(json.loads(line));\n",
    "\n",
    "for data in Dataset:\n",
    "    phraseList.append(data[\"text\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大句长： 124\n",
      "词典大小： 24278\n"
     ]
    }
   ],
   "source": [
    "#统计单词出现频率\n",
    "wordNumDict={};\n",
    "#统计最大句长\n",
    "maxlen=0\n",
    "\n",
    "for phrase in phraseList:\n",
    "    wordList=phrase.split();\n",
    "    if len(wordList) > maxlen:\n",
    "        maxlen=len(wordList)\n",
    "\n",
    "    for word in wordList:\n",
    "        littleword=word.lower() #所有字母转化为小写\n",
    "        if littleword not in wordNumDict:\n",
    "            wordNumDict[littleword]=1;\n",
    "        else:\n",
    "            wordNumDict[littleword]+=1;\n",
    "\n",
    "print('最大句长：',maxlen)\n",
    "print('词典大小：',len(wordNumDict))\n",
    "\n",
    "#词频的字典列表，倒序排列\n",
    "wordFreq=[]\n",
    "\n",
    "output=sorted(wordNumDict.items(),key=lambda x:x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#完成word2idx.json\n",
    "word2idx={}\n",
    "\n",
    "for i in range(len(output)):\n",
    "    word2idx[output[i][0]]=i;\n",
    "\n",
    "with open('Dataset/word2idx.json','w') as f:\n",
    "        json.dump(word2idx, f,ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 tag_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset=[]\n",
    "phraseList=[]\n",
    "\n",
    "with open('Dataset/train.jsonl','r') as f:\n",
    "    for line in f:\n",
    "        Dataset.append(json.loads(line));\n",
    "\n",
    "with open('Dataset/test.jsonl','r') as f:\n",
    "    for line in f:\n",
    "        Dataset.append(json.loads(line));\n",
    "\n",
    "for data in Dataset:\n",
    "    phraseList.append(data[\"NER\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-MISC', 'I-ORG', 'O', 'I-LOC', 'B-MISC', 'B-ORG', 'I-PER', 'B-PER', 'B-LOC'}\n"
     ]
    }
   ],
   "source": [
    "NERtagset=set()\n",
    "for phrase in phraseList:\n",
    "    wordList=phrase.split();\n",
    "    for word in wordList:\n",
    "        if word not in NERtagset:\n",
    "            NERtagset.add(word)\n",
    "print(NERtagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag2idx={'B-PER':0,'I-PER':1,'B-LOC':2,'I-LOC':3,'B-MISC':4,'I-MISC':5,'B-ORG':6,'I-ORG':7,'O':8,\"<START>\":9,\"<STOP>\":10}\n",
    "\n",
    "with open('Dataset/tag2idx.json','w') as f:\n",
    "        json.dump(tag2idx, f,ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a4b0b103f443abab642cb805870998419f99b3b67480212ee124b95ac521b42"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('deep')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
