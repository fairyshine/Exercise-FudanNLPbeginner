{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d1006-57c3-4d6c-9170-0d21e8f2e66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8bd470-c88b-4af5-8bb1-7fd6b852c098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#åˆå§‹åŒ–ç›¸å…³å‚æ•°\n",
    "LIST=['Dataset/list1.csv','Dataset/list2.csv','Dataset/list3.csv'] #ç‰¹å¾è¡¨ç¤ºæ–‡ä»¶çš„åœ°å€\n",
    "\n",
    "C=5            #æ–‡æœ¬åˆ†ç±»çš„ç±»åˆ«\n",
    "w_init=0.01  #æƒé‡å‘é‡çš„åˆå§‹å€¼\n",
    "\n",
    "lr=0.01 #learn rate å­¦ä¹ ç‡\n",
    "\n",
    "feature=[] #ç‰¹å¾å‘é‡ï¼Œå³æƒé‡å‘é‡çš„å¯¹åº”åˆ†é‡ä»£è¡¨çš„ç‰¹å¾\n",
    "\n",
    "for file in LIST:\n",
    "    temp=pd.read_csv(file)\n",
    "    for words in (temp['feat'].tolist()):\n",
    "        words=str(words).split()\n",
    "        feature.append({word for word in words})\n",
    "\n",
    "V=len(feature)  #æƒé‡å‘é‡çš„é•¿åº¦\n",
    "w=pd.DataFrame(np.zeros((C,V))) #æƒé‡å‘é‡è¡¨  \n",
    "w[:]=w_init\n",
    "\n",
    "#ï¼ï¼ğŸŒŸï¼ï¼pandasä¸€ä¸ªç»†èŠ‚ç‚¹ï¼šåˆå§‹åˆ—æ ‡ç­¾çš„æ•°æ®ç±»å‹ä¸ºæ•°å­—ï¼Œä¿å­˜å†è¯»å–åç±»å‹å˜ä¸ºå­—ç¬¦ä¸²äº†\n",
    "#åŸæœ¬ç”¨w[0]è°ƒç”¨ç¬¬ä¸€åˆ—ï¼Œä¿å­˜åéœ€è¦ç”¨w['0']æˆ–w[str(0)]\n",
    "w.to_csv('Dataset/w.csv')\n",
    "\n",
    "w=pd.read_csv('Dataset/w.csv')\n",
    "w=pd.DataFrame(np.array(w)[:,1:]) #ä¿®æ­£å­˜å‚¨æ ¼å¼\n",
    "\n",
    "#æ‰“å°åŒºï¼Œä¾›è‡ªå·±å‚è€ƒ\n",
    "print(\"ç‰¹å¾å‘é‡é•¿åº¦ï¼š\"+str(V))\n",
    "print(feature[-10:])\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6835012d-1271-4edb-a54f-2b93b2512f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateX(sentence):\n",
    "    global feature #å¼•å…¥ç‰¹å¾å‘é‡\n",
    "    x=np.zeros(len(feature))\n",
    "\n",
    "    #ç”Ÿæˆx\n",
    "    sentence=sentence.split(' ')\n",
    "    slen=len(sentence)\n",
    "    for gram in range(1,4):\n",
    "        if slen >= gram:\n",
    "            for i in range(0,slen+1-gram):\n",
    "                wordset = { word.lower() for word in sentence[i:i+gram] }\n",
    "                if wordset in feature:\n",
    "                    x[feature.index(wordset)]+=1\n",
    "    return x\n",
    "\n",
    "\n",
    "def SoftmaxRegression(sentence):\n",
    "    global w #å¼•å…¥æƒé‡å‘é‡\n",
    "    global C\n",
    "    global V\n",
    "    x=np.zeros(V)\n",
    "    y_predict=[0]*C\n",
    "\n",
    "    x=GenerateX(sentence)\n",
    "\n",
    "    #è®¡ç®—å„ç±»åˆ«çš„æ¦‚ç‡\n",
    "    for i in range(0,C):\n",
    "        y_predict[i]= (np.array(w.loc[i])) @ (x.transpose())\n",
    "\n",
    "    return y_predict.index(max(y_predict))\n",
    "\n",
    "\n",
    "SoftmaxRegression('occasionally amuses but none of which amounts to much of a story')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e0a3a-9731-4339-bf64-5d83407932be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_1_epoch():\n",
    "    trainset=pd.read_csv('Dataset/train.tsv',sep='\\t')\n",
    "    global w\n",
    "    global C\n",
    "    global V\n",
    "    global lr\n",
    "    sum_matrix = np.zeros((C,V))\n",
    "    trainsum=int(len(trainset['Phrase'])*0.8)\n",
    "\n",
    "    for i in range(0,2):\n",
    "        y_real=np.zeros(C)\n",
    "        y_real[trainset['Sentiment'][i]]=1\n",
    "\n",
    "        x=np.zeros(V)\n",
    "        x+=GenerateX(trainset['Phrase'][i])\n",
    "        y_predict=np.zeros(C)\n",
    "        for j in range(0,C):\n",
    "            y_predict[j]= (np.array(w.loc[j])) @ (x.transpose())\n",
    "\n",
    "        x=x.reshape(1,V)\n",
    "        y_real=y_real.reshape(1,C)\n",
    "        y_predict=y_predict.reshape(1,C)\n",
    "\n",
    "        sum_matrix += ((y_real-y_predict).transpose()) @ x\n",
    "\n",
    "    w=pd.DataFrame((np.array(w)+lr*1/trainsum*sum_matrix))\n",
    "\n",
    "Train_1_epoch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4d7595-40e5-4f3a-bb8f-29993ba6636d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eval():\n",
    "    testset=pd.read_csv('Dataset/train.tsv',sep='\\t')\n",
    "    end=len(testset['Phrase'])\n",
    "    testnum=int(end*0.2)\n",
    "    goal=0\n",
    "\n",
    "    for i in range(end-testnum,end):\n",
    "        if SoftmaxRegression(testset['Phrase'][i]) == testset['Sentiment'][i]:\n",
    "            goal += 1\n",
    "\n",
    "    return goal/testnum\n",
    "\n",
    "Eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37824544-0490-4f24-8173-b260ca5a065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#å­˜å‚¨å­¦ä¹ æˆæœ\n",
    "w.to_csv('Dataset/w.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ea81e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a4b0b103f443abab642cb805870998419f99b3b67480212ee124b95ac521b42"
  },
  "kernelspec": {
   "display_name": "Python [conda env:deep] *",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
