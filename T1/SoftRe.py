import numpy as np
import pandas as pd

def GenerateX(sentence):
    global feature #å¼•å…¥ç‰¹å¾å‘é‡
    x=np.zeros(len(feature))

    #ç”Ÿæˆx
    sentence=sentence.split(' ')
    slen=len(sentence)
    for gram in range(1,4):
        if slen >= gram:
            for i in range(0,slen+1-gram):
                wordset = { word.lower() for word in sentence[i:i+gram] }
                if wordset in feature:
                    x[feature.index(wordset)]+=1
    return x

def SoftmaxRegression(sentence):
    global w #å¼•å…¥æƒé‡å‘é‡
    global C
    global V
    x=np.zeros(V)
    y_predict=[0]*C

    x=GenerateX(sentence)

    #è®¡ç®—å„ç±»åˆ«çš„æ¦‚ç‡
    for i in range(0,C):
        y_predict[i]= (np.array(w.loc[i])) @ (x.transpose())

    return y_predict.index(max(y_predict))

def Train_1_epoch():
    trainset=pd.read_csv('Dataset/train.tsv',sep='\t')
    global w
    global C
    global V
    global lr
    sum_matrix = np.zeros((C,V))
    trainsum=int(len(trainset['Phrase'])*0.8)

    for i in range(0,trainsum):
        y_real=np.zeros(C)
        y_real[trainset['Sentiment'][i]]=1

        x=np.zeros(V)
        x+=GenerateX(trainset['Phrase'][i])
        y_predict=np.zeros(C)
        for j in range(0,C):
            y_predict[j]= (np.array(w.loc[j])) @ (x.transpose())

        x=x.reshape(1,V)
        y_real=y_real.reshape(1,C)
        y_predict=y_predict.reshape(1,C)

        sum_matrix += ((y_real-y_predict).transpose()) @ x

    w=pd.DataFrame((np.array(w)+lr*1/trainsum*sum_matrix))

def Eval():
    testset=pd.read_csv('Dataset/train.tsv',sep='\t')
    end=len(testset['Phrase'])
    testnum=int(end*0.2)
    goal=0

    for i in range(end-testnum,end):
        if SoftmaxRegression(testset['Phrase'][i]) == testset['Sentiment'][i]:
            goal += 1

    return goal/testnum


#åˆå§‹åŒ–ç›¸å…³å‚æ•°
LIST=['Dataset/list1.csv','Dataset/list2.csv','Dataset/list3.csv'] #ç‰¹å¾è¡¨ç¤ºæ–‡ä»¶çš„åœ°å€

C=5            #æ–‡æœ¬åˆ†ç±»çš„ç±»åˆ«
w_init=0.1  #æƒé‡å‘é‡çš„åˆå§‹å€¼

lr=0.01 #learn rate å­¦ä¹ ç‡

feature=[] #ç‰¹å¾å‘é‡ï¼Œå³æƒé‡å‘é‡çš„å¯¹åº”åˆ†é‡ä»£è¡¨çš„ç‰¹å¾

for file in LIST:
    temp=pd.read_csv(file)
    for words in (temp['feat'].tolist()):
        words=str(words).split()
        feature.append({word for word in words})

V=len(feature)  #æƒé‡å‘é‡çš„é•¿åº¦
w=pd.DataFrame(np.zeros((C,V))) #æƒé‡å‘é‡è¡¨
w[:]=w_init

#ï¼ï¼ğŸŒŸï¼ï¼pandasä¸€ä¸ªç»†èŠ‚ç‚¹ï¼šåˆå§‹åˆ—æ ‡ç­¾çš„æ•°æ®ç±»å‹ä¸ºæ•°å­—ï¼Œä¿å­˜å†è¯»å–åç±»å‹å˜ä¸ºå­—ç¬¦ä¸²äº†
#åŸæœ¬ç”¨w[0]è°ƒç”¨ç¬¬ä¸€åˆ—ï¼Œä¿å­˜åéœ€è¦ç”¨w['0']æˆ–w[str(0)]
w.to_csv('Dataset/w.csv')

w=pd.read_csv('Dataset/w.csv')
w=pd.DataFrame(np.array(w)[:,1:]) #ä¿®æ­£å­˜å‚¨æ ¼å¼

flag=1
epochs=1
score=0
score_old=-1
while flag:
    Train_1_epoch()
    score=Eval()
    print("epoch"+str(epochs)+":"+str(score))
    if score-score_old <0.01 and epochs>=10 :
        flag=0
    else:
        score_old=score
        epochs+=1

#å­˜å‚¨å­¦ä¹ æˆæœ
w.to_csv('Dataset/w.csv')

